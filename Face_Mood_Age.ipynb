{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35209d84-9d69-4ef1-8355-e7ce92646360",
   "metadata": {},
   "source": [
    "### Step 1: Setting Up the Environment\n",
    "#### Install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be37857-3847-4058-b419-f640a6a7a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow keras opencv-python scikit-learn flask flask-cors numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0239c-27a7-43e4-b46a-b8a196a7ffc6",
   "metadata": {},
   "source": [
    "### Step 2: Download and Prepare the Datasets\n",
    "#### Mood: Data set : https://www.kaggle.com/datasets/msambare/fer2013\n",
    "#### Labelled Faces in the Wild (LFW) Dataset : https://www.kaggle.com/datasets/jessicali9530/lfw-dataset?resource=download\n",
    "#### Age DB: https://www.kaggle.com/datasets/nitingandhi/agedb-database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae7c39-308c-45c3-9257-b59b80a81eea",
   "metadata": {},
   "source": [
    "### Step 3: Training the Models\n",
    "#### 3.1 Training a Face Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "436f3dfe-7232-41ae-9509-423b08493543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13233 images from Labelled Faces in the Wild (LFW) Dataset/lfw/lfw-deepfunneled\n",
      "Epoch 1/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 59ms/step - accuracy: 0.0490 - loss: 8.1422 - val_accuracy: 0.0000e+00 - val_loss: 10.0848\n",
      "Epoch 2/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 51ms/step - accuracy: 0.0509 - loss: 7.3915 - val_accuracy: 0.0000e+00 - val_loss: 11.6124\n",
      "Epoch 3/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 51ms/step - accuracy: 0.0750 - loss: 6.9540 - val_accuracy: 0.0000e+00 - val_loss: 12.5301\n",
      "Epoch 4/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 51ms/step - accuracy: 0.1070 - loss: 6.4077 - val_accuracy: 0.0000e+00 - val_loss: 14.4490\n",
      "Epoch 5/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 52ms/step - accuracy: 0.1350 - loss: 5.7958 - val_accuracy: 0.0000e+00 - val_loss: 16.5771\n",
      "Epoch 6/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 52ms/step - accuracy: 0.1940 - loss: 4.8553 - val_accuracy: 0.0000e+00 - val_loss: 19.8555\n",
      "Epoch 7/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 53ms/step - accuracy: 0.2521 - loss: 3.8399 - val_accuracy: 0.0000e+00 - val_loss: 25.5541\n",
      "Epoch 8/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 52ms/step - accuracy: 0.4195 - loss: 2.7045 - val_accuracy: 0.0000e+00 - val_loss: 30.8930\n",
      "Epoch 9/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 54ms/step - accuracy: 0.5669 - loss: 1.8713 - val_accuracy: 0.0000e+00 - val_loss: 35.8231\n",
      "Epoch 10/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 53ms/step - accuracy: 0.6949 - loss: 1.2497 - val_accuracy: 0.0000e+00 - val_loss: 38.5581\n",
      "Epoch 11/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 53ms/step - accuracy: 0.7925 - loss: 0.8268 - val_accuracy: 0.0000e+00 - val_loss: 43.6429\n",
      "Epoch 12/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 53ms/step - accuracy: 0.8568 - loss: 0.5678 - val_accuracy: 0.0000e+00 - val_loss: 46.9529\n",
      "Epoch 13/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 53ms/step - accuracy: 0.8953 - loss: 0.4089 - val_accuracy: 0.0000e+00 - val_loss: 52.0965\n",
      "Epoch 14/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 53ms/step - accuracy: 0.9228 - loss: 0.2975 - val_accuracy: 0.0000e+00 - val_loss: 53.2965\n",
      "Epoch 15/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 54ms/step - accuracy: 0.9445 - loss: 0.2231 - val_accuracy: 0.0000e+00 - val_loss: 57.4367\n",
      "Epoch 16/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.9535 - loss: 0.1789 - val_accuracy: 0.0000e+00 - val_loss: 61.0508\n",
      "Epoch 17/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - accuracy: 0.9514 - loss: 0.1723 - val_accuracy: 0.0000e+00 - val_loss: 58.8126\n",
      "Epoch 18/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 53ms/step - accuracy: 0.9554 - loss: 0.1623 - val_accuracy: 0.0000e+00 - val_loss: 60.8493\n",
      "Epoch 19/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 53ms/step - accuracy: 0.9646 - loss: 0.1304 - val_accuracy: 0.0000e+00 - val_loss: 65.8779\n",
      "Epoch 20/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 54ms/step - accuracy: 0.9640 - loss: 0.1327 - val_accuracy: 0.0000e+00 - val_loss: 63.7232\n",
      "Epoch 21/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 53ms/step - accuracy: 0.9690 - loss: 0.1079 - val_accuracy: 0.0000e+00 - val_loss: 67.9844\n",
      "Epoch 22/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 54ms/step - accuracy: 0.9735 - loss: 0.0884 - val_accuracy: 0.0000e+00 - val_loss: 65.0207\n",
      "Epoch 23/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - accuracy: 0.9662 - loss: 0.1118 - val_accuracy: 0.0000e+00 - val_loss: 64.5321\n",
      "Epoch 24/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 54ms/step - accuracy: 0.9664 - loss: 0.1146 - val_accuracy: 0.0000e+00 - val_loss: 64.4629\n",
      "Epoch 25/25\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - accuracy: 0.9632 - loss: 0.1167 - val_accuracy: 0.0000e+00 - val_loss: 65.3666\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def load_lfw_data(data_dir):\n",
    "    X = []\n",
    "    y = []\n",
    "    label_map = {}\n",
    "    current_label = 0\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Directory {data_dir} does not exist\")\n",
    "        return np.array(X), np.array(y), label_map\n",
    "    \n",
    "    for person_name in os.listdir(data_dir):\n",
    "        person_dir = os.path.join(data_dir, person_name)\n",
    "        if os.path.isdir(person_dir):\n",
    "            for image_name in os.listdir(person_dir):\n",
    "                image_path = os.path.join(person_dir, image_name)\n",
    "                image = cv2.imread(image_path)\n",
    "                if image is not None:\n",
    "                    image = cv2.resize(image, (64, 64))  # Resize to 64x64\n",
    "                    X.append(image)\n",
    "                    if person_name not in label_map:\n",
    "                        label_map[person_name] = current_label\n",
    "                        current_label += 1\n",
    "                    y.append(label_map[person_name])\n",
    "                else:\n",
    "                    print(f\"Failed to read image: {image_path}\")\n",
    "    \n",
    "    if not X:\n",
    "        print(\"No images loaded.\")\n",
    "    else:\n",
    "        print(f\"Loaded {len(X)} images from {data_dir}\")\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y, label_map\n",
    "\n",
    "# Load the LFW dataset\n",
    "lfw_data_dir = 'Labelled Faces in the Wild (LFW) Dataset/lfw/lfw-deepfunneled'  # Replace with the path to your LFW data directory\n",
    "X, y, lfw_label_map = load_lfw_data(lfw_data_dir)\n",
    "\n",
    "if X.size == 0 or y.size == 0:\n",
    "    print(\"No data loaded, please check the dataset directory and files.\")\n",
    "else:\n",
    "    # Normalize the data\n",
    "    X = X / 255.0\n",
    "\n",
    "    # Define a simple CNN model for face detection\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(len(np.unique(y)), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X, y, epochs=22, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model on the validation set - not working\n",
    "    # val_loss, val_accuracy = model.evaluate(val_generator)\n",
    "    # print(\"Validation Loss:\", val_loss)\n",
    "    # print(\"Validation Accuracy:\", val_accuracy)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save('face_detection_model.keras') #.h5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5655f4-ffd7-4413-8a53-16b82a0422e7",
   "metadata": {},
   "source": [
    "#### 3.2 Training an Age Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d4ca555-904b-419f-b452-866a23152270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 59ms/step - loss: 42875248.0000 - mae: 5458.2280 - val_loss: 9512725.0000 - val_mae: 2072.9163\n",
      "Epoch 2/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - loss: 25834510.0000 - mae: 4356.7695 - val_loss: 8798625.0000 - val_mae: 2156.3459\n",
      "Epoch 3/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - loss: 24133652.0000 - mae: 4152.4844 - val_loss: 10019715.0000 - val_mae: 2622.2415\n",
      "Epoch 4/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 23815758.0000 - mae: 4114.8193 - val_loss: 8382102.0000 - val_mae: 2144.6680\n",
      "Epoch 5/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - loss: 24035932.0000 - mae: 4136.8198 - val_loss: 9014564.0000 - val_mae: 2220.6028\n",
      "Epoch 6/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - loss: 23134036.0000 - mae: 4047.0549 - val_loss: 8292337.0000 - val_mae: 2032.4668\n",
      "Epoch 7/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - loss: 22639680.0000 - mae: 4026.2153 - val_loss: 8446751.0000 - val_mae: 2078.0125\n",
      "Epoch 8/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 45ms/step - loss: 22765472.0000 - mae: 4034.1567 - val_loss: 8700557.0000 - val_mae: 2131.3215\n",
      "Epoch 9/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 53ms/step - loss: 22380608.0000 - mae: 4015.2542 - val_loss: 8274322.0000 - val_mae: 2103.7939\n",
      "Epoch 10/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - loss: 22457680.0000 - mae: 4016.9429 - val_loss: 9742514.0000 - val_mae: 2155.5933\n",
      "Epoch 11/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 45ms/step - loss: 21769522.0000 - mae: 3927.4219 - val_loss: 8299740.5000 - val_mae: 2130.1860\n",
      "Epoch 12/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 45ms/step - loss: 21083554.0000 - mae: 3842.1157 - val_loss: 8801093.0000 - val_mae: 2212.4238\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 18618844.0000 - mae: 3566.0034\n",
      "Train Loss: 19117422.0\n",
      "Train MAE: 3591.60791015625\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Function to load AgeDB data\n",
    "def load_age_data(data_dir):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for image_name in os.listdir(data_dir):\n",
    "        image_path = os.path.join(data_dir, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, (64, 64))  # Resize to 64x64\n",
    "            X.append(image)\n",
    "            age = int(image_name.split(\"_\")[0])  # Extract age from the filename\n",
    "            y.append(age)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Load the AgeDB dataset\n",
    "agedb_data_dir = 'Age dataset\\AgeDB'  # Replace with the path to your AgeDB data directory\n",
    "X, y = load_age_data(agedb_data_dir)\n",
    "\n",
    "if X.size == 0 or y.size == 0:\n",
    "    print(\"No data loaded, please check the dataset directory and files.\")\n",
    "else:\n",
    "    # Normalize the data\n",
    "    X = X / 255.0\n",
    "\n",
    "    # Define a CNN model for age prediction\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1)  # Single output for age\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "    # Train the model\n",
    "    # model.fit(X, y, epochs=10, validation_split=0.2)\n",
    "    \n",
    "    # Train the model with early stopping based on validation loss\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    model.fit(X, y, epochs=30, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    # Save the model\n",
    "    model.save('age_prediction_model.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb24f6-5fca-4b42-b74e-9641b6690aeb",
   "metadata": {},
   "source": [
    "#### The values \"Train Loss\" and \"Train MAE\" represent the performance metrics of your trained age prediction model on the training dataset. Here's what each of them means:\n",
    "\n",
    "##### Train Loss (Mean Squared Error):\n",
    "\n",
    "##### The \"Train Loss\" value of 19117422.0 represents the mean squared error (MSE) of your model's predictions on the training dataset.\n",
    "##### MSE is calculated by averaging the squared differences between the actual ages and the ages predicted by your model.\n",
    "##### In this case, a Train Loss of 19117422.0 indicates that, on average, the squared difference between the predicted ages and the actual ages is approximately 19117422.0.\n",
    "##### Lower values of MSE indicate better model performance, as they suggest that the model's predictions are closer to the actual ages.\n",
    "\n",
    "#### Train MAE (Mean Absolute Error):\n",
    "\n",
    "##### The \"Train MAE\" value of 3591.60791015625 represents the mean absolute error (MAE) of your model's predictions on the training dataset.\n",
    "##### MAE is calculated by averaging the absolute differences between the actual ages and the ages predicted by your model.\n",
    "##### In this case, a Train MAE of 3591.60791015625 indicates that, on average, the absolute difference between the predicted ages and the actual ages is approximately 3591.60791015625 years.\n",
    "##### Lower values of MAE also indicate better model performance, as they suggest that the model's predictions are closer to the actual ages, on average.\n",
    "\n",
    "#### Interpretation:\n",
    "##### The values of Train Loss and Train MAE provide insights into how well your model is performing on the training dataset. Lower values indicate better performance, as they suggest that the model's predictions are closer to the actual ages.\n",
    "##### It's essential to interpret these values in the context of your specific application and domain knowledge. For instance, consider the scale of the target variable (age) and the implications of prediction errors for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3befabe7-d69d-48fa-af0b-c2fbc3374f91",
   "metadata": {},
   "source": [
    "#### 3.3 Training a Mood Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8c102ee-16ff-4d81-a5d2-f7938344ac29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "Label Encoder classes: ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'surprise']\n",
      "Processing directory: C:\\Users\\13ics\\Bathxpertx\\face recognisition\\Mood dataset\\All\\angry\n",
      "Processing directory: C:\\Users\\13ics\\Bathxpertx\\face recognisition\\Mood dataset\\All\\disgust\n",
      "Processing directory: C:\\Users\\13ics\\Bathxpertx\\face recognisition\\Mood dataset\\All\\fear\n",
      "Processing directory: C:\\Users\\13ics\\Bathxpertx\\face recognisition\\Mood dataset\\All\\happy\n",
      "Processing directory: C:\\Users\\13ics\\Bathxpertx\\face recognisition\\Mood dataset\\All\\neutral\n",
      "Processing directory: C:\\Users\\13ics\\Bathxpertx\\face recognisition\\Mood dataset\\All\\sad\n",
      "Processing directory: C:\\Users\\13ics\\Bathxpertx\\face recognisition\\Mood dataset\\All\\surprise\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 631. MiB for an array with shape (35887, 48, 48) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Load the FER-2013 dataset\u001b[39;00m\n\u001b[0;32m     56\u001b[0m fer2013_data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m13ics\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBathxpertx\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mface recognisition\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMood dataset\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Replace with the path to your FER-2013 dataset directory\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m X, y \u001b[38;5;241m=\u001b[39m load_mood_data(fer2013_data_dir)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Check if data is loaded correctly\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of X:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[59], line 53\u001b[0m, in \u001b[0;36mload_mood_data\u001b[1;34m(data_dir)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot a valid file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 631. MiB for an array with shape (35887, 48, 48) and data type float64"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Function to load FER-2013 data\n",
    "def load_mood_data(data_dir):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = []\n",
    "\n",
    "    # Collect all labels to fit the label encoder\n",
    "    for emotion in os.listdir(data_dir):\n",
    "        emotion_dir = os.path.join(data_dir, emotion)\n",
    "        if os.path.isdir(emotion_dir):\n",
    "            labels.append(emotion)\n",
    "    label_encoder.fit(labels)\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Label Encoder classes:\", label_encoder.classes_)\n",
    "\n",
    "    # Iterate through each emotion directory\n",
    "    for emotion in os.listdir(data_dir):\n",
    "        emotion_dir = os.path.join(data_dir, emotion)\n",
    "        if os.path.isdir(emotion_dir):\n",
    "            print(f\"Processing directory: {emotion_dir}\")\n",
    "            # Load images from each emotion subdirectory\n",
    "            for filename in os.listdir(emotion_dir):\n",
    "                image_path = os.path.join(emotion_dir, filename)\n",
    "                if os.path.isfile(image_path):\n",
    "                    # Read image and resize to (48, 48)\n",
    "                    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    if image is not None:\n",
    "                        image = cv2.resize(image, (48, 48))\n",
    "                        # Normalize pixel values to range [0, 1]\n",
    "                        image = image / 255.0\n",
    "                        # Encode emotion label\n",
    "                        label = label_encoder.transform([emotion])[0]\n",
    "                        # Append image and label to X and y\n",
    "                        X.append(image)\n",
    "                        y.append(label)\n",
    "                    else:\n",
    "                        print(f\"Failed to load image: {image_path}\")\n",
    "                else:\n",
    "                    print(f\"Not a valid file: {image_path}\")\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load the FER-2013 dataset\n",
    "fer2013_data_dir = r'C:\\Users\\13ics\\Bathxpertx\\face recognisition\\Mood dataset\\All' # Replace with the path to your FER-2013 dataset directory\n",
    "X, y = load_mood_data(fer2013_data_dir)\n",
    "\n",
    "# Check if data is loaded correctly\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Ensure that we have loaded some images\n",
    "if X.shape[0] == 0 or y.shape[0] == 0:\n",
    "    raise ValueError(\"No images loaded. Please check the dataset directory and image paths.\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Expand dimensions to match the input shape of the model\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# Define a CNN model for mood detection with dropout layers\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with validation split and monitor training process\n",
    "history = model.fit(X_train, y_train, epochs=75, validation_split=0.5)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Save the model\n",
    "model.save('mood_detection_model.keras')\n",
    "\n",
    "# Plot training and validation accuracy/loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d1a0541-3145-47d7-9f2a-d4525340ab2f",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Function to load FER-2013 data\n",
    "def load_mood_data(data_dir):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = []\n",
    "\n",
    "    # Collect all labels to fit the label encoder\n",
    "    for emotion in os.listdir(data_dir):\n",
    "        emotion_dir = os.path.join(data_dir, emotion)\n",
    "        if os.path.isdir(emotion_dir):\n",
    "            labels.append(emotion)\n",
    "    label_encoder.fit(labels)\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Label Encoder classes:\", label_encoder.classes_)\n",
    "\n",
    "    # Iterate through each emotion directory\n",
    "    for emotion in os.listdir(data_dir):\n",
    "        emotion_dir = os.path.join(data_dir, emotion)\n",
    "        if os.path.isdir(emotion_dir):\n",
    "            print(f\"Processing directory: {emotion_dir}\")\n",
    "            # Load images from each emotion subdirectory\n",
    "            for filename in os.listdir(emotion_dir):\n",
    "                image_path = os.path.join(emotion_dir, filename)\n",
    "                if os.path.isfile(image_path):\n",
    "                    # print(f\"Found file: {image_path}\")\n",
    "                    # Read image and resize to (48, 48)\n",
    "                    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    if image is not None:\n",
    "                        image = cv2.resize(image, (48, 48))\n",
    "                        # Normalize pixel values to range [0, 1]\n",
    "                        image = image / 255.0\n",
    "                        # Encode emotion label\n",
    "                        label = label_encoder.transform([emotion])[0]\n",
    "                        # Append image and label to X and y\n",
    "                        X.append(image)\n",
    "                        y.append(label)\n",
    "                    else:\n",
    "                        print(f\"Failed to load image: {image_path}\")\n",
    "                else:\n",
    "                    print(f\"Not a valid file: {image_path}\")\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load the FER-2013 dataset\n",
    "fer2013_data_dir = 'C:/Users/13ics/Bathxpertx/face recognisition/Mood dataset/All'  # Replace with the path to your FER-2013 dataset directory\n",
    "X, y = load_mood_data(fer2013_data_dir)\n",
    "\n",
    "# Check if data is loaded correctly\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Ensure that we have loaded some images\n",
    "if X.shape[0] == 0 or y.shape[0] == 0:\n",
    "    raise ValueError(\"No images loaded. Please check the dataset directory and image paths.\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Expand dimensions to match the input shape of the model\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# Define data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit the generator on the training data\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Define a CNN model for mood detection with dropout and L2 regularization\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001), input_shape=(48, 48, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(np.unique(y)), activation='softmax', kernel_regularizer=l2(0.001))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with data augmentation\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=50),\n",
    "          epochs=50,  # Increased epochs to allow more time for learning\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=2)  # Reduce verbosity to 2\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Save the model\n",
    "model.save('mood_detection_model.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211192ca-6e9c-4602-9fbf-92eef87d0510",
   "metadata": {},
   "source": [
    "### Step 4: Set Up the Flask Backend\n",
    "#### 4.1 Create Flask App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9065437-6721-43bc-9a8b-0ea850a9bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Flask flask_cors opencv-python tensorflow numpy\n",
    "# !pip install --upgrade flask flask-cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "83ac6c2d-958e-4cb5-8799-907141a2f8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug: * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Load models\n",
    "face_model = tf.keras.models.load_model('face_detection_model.keras')\n",
    "age_model = tf.keras.models.load_model('age_prediction_model.keras')\n",
    "mood_model = tf.keras.models.load_model('mood_detection_model.keras')\n",
    "\n",
    "# Load face detection cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier('path/to/haarcascade_frontalface_default.xml')\n",
    "\n",
    "@app.route('/detect', methods=['POST'])\n",
    "def detect():\n",
    "    file = request.files['image']\n",
    "    image = cv2.imdecode(np.frombuffer(file.read(), np.uint8), cv2.IMREAD_COLOR)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    results = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = image[y:y+h, x:x+w]\n",
    "        age = predict_age(face)\n",
    "        mood = predict_mood(face)\n",
    "        name = predict_name(face)  # Implement similar to age and mood prediction\n",
    "        results.append({'age': age, 'mood': mood, 'name': name})\n",
    "\n",
    "    return jsonify(results)\n",
    "\n",
    "def predict_age(face):\n",
    "    face_resized = cv2.resize(face, (64, 64)) / 255.0\n",
    "    face_resized = np.expand_dims(face_resized, axis=0)\n",
    "    age = age_model.predict(face_resized)\n",
    "    return age[0][0]\n",
    "\n",
    "def predict_mood(face):\n",
    "    face_resized = cv2.resize(face, (48, 48)) / 255.0\n",
    "    face_resized = np.expand_dims(face_resized, axis=0)\n",
    "    mood = mood_model.predict(face_resized)\n",
    "    return np.argmax(mood)\n",
    "\n",
    "def predict_name(face):\n",
    "    face_resized = cv2.resize(face, (224, 224)) / 255.0\n",
    "    face_resized = np.expand_dims(face_resized, axis=0)\n",
    "    name = name_model.predict(face_resized)\n",
    "    return np.argmax(name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "    app.run(use_reloader=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d2e31-4338-464d-af95-ff4f14d94a76",
   "metadata": {},
   "source": [
    "### Step 5: Create the Frontend\n",
    "#### 5.1 Basic HTML Structure"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7db36da5-1d9d-4770-99bc-e372c43e0618",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Face Detection App</title>\n",
    "    <style>\n",
    "        /* Add your CSS styles here */\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Face Detection App</h1>\n",
    "    <div id=\"upload-section\">\n",
    "        <h2>Upload a Picture</h2>\n",
    "        <input type=\"file\" id=\"upload-image\" accept=\"image/*\">\n",
    "        <button onclick=\"uploadImage()\">Detect</button>\n",
    "    </div>\n",
    "    <div id=\"results-section\">\n",
    "        <!-- Results will be displayed here -->\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        async function uploadImage() {\n",
    "            const file = document.getElementById('upload-image').files[0];\n",
    "            const formData = new FormData();\n",
    "            formData.append('image', file);\n",
    "\n",
    "            const response = await fetch('http://localhost:5000/detect', {\n",
    "                method: 'POST',\n",
    "                body: formData\n",
    "            });\n",
    "            const data = await response.json();\n",
    "            displayResults(data);\n",
    "        }\n",
    "\n",
    "        function displayResults(data) {\n",
    "            const resultsSection = document.getElementById('results-section');\n",
    "            resultsSection.innerHTML = '';\n",
    "\n",
    "            data.forEach(result => {\n",
    "                const resultDiv = document.createElement('div');\n",
    "                resultDiv.innerHTML = `Age: ${result.age}, Mood: ${result.mood}, Name: ${result.name}`;\n",
    "                resultsSection.appendChild(resultDiv);\n",
    "            });\n",
    "        }\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b95c0-3172-4843-a9f7-41622bf3fe49",
   "metadata": {},
   "source": [
    "### Step 6: Running the Application\n",
    "#### 6.1 Start the Flask Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5066c79a-3680-4935-8753-9c2eec945bbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (945115591.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[52], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python app.py\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57998bcb-f93f-475f-8477-d5618d037244",
   "metadata": {},
   "source": [
    "### 6.2 Open the HTML File in Your Browser\n",
    "#### You can open the HTML file in a web browser to interact with your face detection app.\n",
    "\n",
    "### Conclusion\n",
    "#### This guide provides a comprehensive framework for building a face detection application with an attractive UI, including training models and \n",
    "#### deploying them with Flask. You can extend this by adding more functionalities and improving the models' accuracy by using larger datasets and \n",
    "#### fine-tuning the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
